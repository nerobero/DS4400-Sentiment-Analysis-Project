{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e292b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries here: \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, auc, roc_auc_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f58ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>This book was the very first bookmobile book I...</td>\n",
       "      <td>50 + years ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>When I read the description for this book, I c...</td>\n",
       "      <td>Boring! Boring! Boring!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>I just had to edit this review. This book is a...</td>\n",
       "      <td>Wiggleliscious/new toy ready/!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>I don't normally buy 'mystery' novels because ...</td>\n",
       "      <td>Very good read.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>This isn't the kind of book I normally read, a...</td>\n",
       "      <td>Great Story!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>11995</td>\n",
       "      <td>2</td>\n",
       "      <td>Had to read certain passages twice--typos.  Wi...</td>\n",
       "      <td>Where's the meat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>11996</td>\n",
       "      <td>3</td>\n",
       "      <td>Not what i expected. yet a very interesting bo...</td>\n",
       "      <td>Interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>11997</td>\n",
       "      <td>5</td>\n",
       "      <td>Dragon Knights is a world where Knights ride d...</td>\n",
       "      <td>Dragon Knights, Wings of Change (I Dream of Dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>11998</td>\n",
       "      <td>4</td>\n",
       "      <td>Since this story is very short, it's hard to s...</td>\n",
       "      <td>Good writing, short story</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>11999</td>\n",
       "      <td>4</td>\n",
       "      <td>from 1922 an amazing collection of info on sym...</td>\n",
       "      <td>interesting public domain book</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  rating                                         reviewText  \\\n",
       "0               0       5  This book was the very first bookmobile book I...   \n",
       "1               1       1  When I read the description for this book, I c...   \n",
       "2               2       5  I just had to edit this review. This book is a...   \n",
       "3               3       5  I don't normally buy 'mystery' novels because ...   \n",
       "4               4       5  This isn't the kind of book I normally read, a...   \n",
       "...           ...     ...                                                ...   \n",
       "11995       11995       2  Had to read certain passages twice--typos.  Wi...   \n",
       "11996       11996       3  Not what i expected. yet a very interesting bo...   \n",
       "11997       11997       5  Dragon Knights is a world where Knights ride d...   \n",
       "11998       11998       4  Since this story is very short, it's hard to s...   \n",
       "11999       11999       4  from 1922 an amazing collection of info on sym...   \n",
       "\n",
       "                                                 summary  \n",
       "0                                      50 + years ago...  \n",
       "1                                Boring! Boring! Boring!  \n",
       "2                        Wiggleliscious/new toy ready/!!  \n",
       "3                                        Very good read.  \n",
       "4                                           Great Story!  \n",
       "...                                                  ...  \n",
       "11995                                  Where's the meat?  \n",
       "11996                                        Interesting  \n",
       "11997  Dragon Knights, Wings of Change (I Dream of Dr...  \n",
       "11998                          Good writing, short story  \n",
       "11999                     interesting public domain book  \n",
       "\n",
       "[12000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "amazon = pd.read_csv(\"Data/kindle_review.csv\")\n",
    "amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6991401e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Distribution\n",
    "amazon.rating.value_counts().sort_index().plot(kind='bar',color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8504fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = amazon['reviewText']\n",
    "y = amazon['rating']\n",
    "\n",
    "# Split train_data into testing and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, random_state=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf18938",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c41a87",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasmineliu0114/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.0\n",
      "Training score: 0.999375\n",
      "Val score: 0.41458333333333336\n",
      "F1 score: 0.38991582501251576\n",
      "alpha: 0.1\n",
      "Training score: 0.9958333333333333\n",
      "Val score: 0.47291666666666665\n",
      "F1 score: 0.4276901325528718\n",
      "alpha: 0.2\n",
      "Training score: 0.9755208333333333\n",
      "Val score: 0.435\n",
      "F1 score: 0.37623183362968227\n",
      "alpha: 0.30000000000000004\n",
      "Training score: 0.9214583333333334\n",
      "Val score: 0.41458333333333336\n",
      "F1 score: 0.34505139263371226\n",
      "alpha: 0.4\n",
      "Training score: 0.86\n",
      "Val score: 0.40541666666666665\n",
      "F1 score: 0.32820412167533575\n",
      "alpha: 0.5\n",
      "Training score: 0.8046875\n",
      "Val score: 0.3975\n",
      "F1 score: 0.3128675284872132\n",
      "alpha: 0.6000000000000001\n",
      "Training score: 0.7539583333333333\n",
      "Val score: 0.38875\n",
      "F1 score: 0.2965297320669956\n",
      "alpha: 0.7000000000000001\n",
      "Training score: 0.7125\n",
      "Val score: 0.38333333333333336\n",
      "F1 score: 0.28743466523697564\n",
      "alpha: 0.8\n",
      "Training score: 0.6784375\n",
      "Val score: 0.3775\n",
      "F1 score: 0.27680023974909845\n",
      "alpha: 0.9\n",
      "Training score: 0.650625\n",
      "Val score: 0.3720833333333333\n",
      "F1 score: 0.26779403362673443\n",
      "alpha: 1.0\n",
      "Training score: 0.6271875\n",
      "Val score: 0.37041666666666667\n",
      "F1 score: 0.2641252723792353\n"
     ]
    }
   ],
   "source": [
    "# Range of alpha values from 0-1\n",
    "alpha = np.arange(0,1.1,0.1)\n",
    "\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_val based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "#Iterate through each alpha value\n",
    "for a in alpha:\n",
    "    # Define a MultinomialNB model with current alpha value\n",
    "    mnb = MultinomialNB(alpha = a).fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = mnb.predict(X_train_vectorized)\n",
    "    val_pred = mnb.predict(X_val_vectorized)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    train_error = accuracy_score(y_train, train_pred)\n",
    "    val_error = accuracy_score(y_val, val_pred)\n",
    "    f1 = f1_score(y_val, val_pred, average='macro')\n",
    "    \n",
    "    print(f'alpha: {a}')\n",
    "    print(f'Training score: {train_error}')\n",
    "    print(f'Val score: {val_error}')\n",
    "    print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23a3bd",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8710aef",
   "metadata": {},
   "source": [
    "### One-vs-Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5dce0f9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train error: 0.10364583333333333\n",
      "test error: 0.48624999999999996\n",
      "F1 score: 0.48770011238311783\n",
      "1\n",
      "train error: 0.10364583333333333\n",
      "test error: 0.48624999999999996\n",
      "F1 score: 0.48770011238311783\n",
      "2\n",
      "train error: 0.13406249999999997\n",
      "test error: 0.48416666666666663\n",
      "F1 score: 0.48822465349308625\n",
      "3\n",
      "train error: 0.1511458333333333\n",
      "test error: 0.4816666666666667\n",
      "F1 score: 0.4923143078194438\n",
      "4\n",
      "train error: 0.16312499999999996\n",
      "test error: 0.48750000000000004\n",
      "F1 score: 0.4866250953345087\n",
      "5\n",
      "train error: 0.17197916666666668\n",
      "test error: 0.48583333333333334\n",
      "F1 score: 0.48785617974044093\n",
      "6\n",
      "train error: 0.18041666666666667\n",
      "test error: 0.48750000000000004\n",
      "F1 score: 0.48717869042151934\n",
      "7\n",
      "train error: 0.1869791666666667\n",
      "test error: 0.48958333333333337\n",
      "F1 score: 0.4849772709425877\n",
      "8\n",
      "train error: 0.19281250000000005\n",
      "test error: 0.4870833333333333\n",
      "F1 score: 0.4876138833361317\n",
      "9\n",
      "train error: 0.19843750000000004\n",
      "test error: 0.4883333333333333\n",
      "F1 score: 0.4870891151864976\n",
      "10\n",
      "train error: 0.20281249999999995\n",
      "test error: 0.49124999999999996\n",
      "F1 score: 0.48582650785947507\n"
     ]
    }
   ],
   "source": [
    "# Range of min_df values from 0-10\n",
    "min_df = range(11)\n",
    "\n",
    "# Iterate through min_df values\n",
    "for m in min_df:\n",
    "    #create the vocabulary based on the training data\n",
    "    vect = TfidfVectorizer(min_df=m, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "    #encode the words in X_train and X_val based on the vocabulary\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "    # Pass a LogisticRegression model through a OneVsRestClassifier object\n",
    "    ovr = OneVsRestClassifier(LogisticRegression(n_jobs = -1))\n",
    "    ovr.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = ovr.predict(X_train_vectorized)\n",
    "    val_pred = ovr.predict(X_val_vectorized)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    train_score = accuracy_score(train_pred, y_train)\n",
    "    val_score = accuracy_score(val_pred, y_val)\n",
    "    f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "    print(m)\n",
    "    print(f\"train error: {1-train_score}\")\n",
    "    print(f\"test error: {1-val_score}\")\n",
    "    print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6350c8ad",
   "metadata": {},
   "source": [
    "### One-vs-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa63886b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train error: 0.14718750000000003\n",
      "test error: 0.4979166666666667\n",
      "F1 score: 0.4777461333324117\n",
      "1\n",
      "train error: 0.14718750000000003\n",
      "test error: 0.4979166666666667\n",
      "F1 score: 0.4777461333324117\n",
      "2\n",
      "train error: 0.17729166666666663\n",
      "test error: 0.49291666666666667\n",
      "F1 score: 0.48273185362490256\n",
      "3\n",
      "train error: 0.1908333333333333\n",
      "test error: 0.48750000000000004\n",
      "F1 score: 0.4890071632071617\n",
      "4\n",
      "train error: 0.20020833333333332\n",
      "test error: 0.49083333333333334\n",
      "F1 score: 0.4870153752322424\n",
      "5\n",
      "train error: 0.20697916666666671\n",
      "test error: 0.49124999999999996\n",
      "F1 score: 0.4874473404757638\n",
      "6\n",
      "train error: 0.2136458333333333\n",
      "test error: 0.48958333333333337\n",
      "F1 score: 0.48936268050064086\n",
      "7\n",
      "train error: 0.21999999999999997\n",
      "test error: 0.4883333333333333\n",
      "F1 score: 0.490790446429194\n",
      "8\n",
      "train error: 0.2242708333333333\n",
      "test error: 0.48875\n",
      "F1 score: 0.4906374455286545\n",
      "9\n",
      "train error: 0.23020833333333335\n",
      "test error: 0.48916666666666664\n",
      "F1 score: 0.4901985040245023\n",
      "10\n",
      "train error: 0.2348958333333333\n",
      "test error: 0.4854166666666667\n",
      "F1 score: 0.49426175217768914\n"
     ]
    }
   ],
   "source": [
    "# Range of min_df values from 0-10\n",
    "min_df = range(11)\n",
    "\n",
    "# Iterate through min_df values\n",
    "for m in min_df:\n",
    "    #create the vocabulary based on the training data\n",
    "    vect = TfidfVectorizer(min_df=m, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "    #encode the words in X_train and X_val based on the vocabulary\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "    # Pass a LogisticRegression model through a OneVsOneClassifier object\n",
    "    ovo = OneVsOneClassifier(LogisticRegression(n_jobs = -1))\n",
    "    ovo.fit(X_train_vectorized, y_train)\n",
    "    ovo.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = ovo.predict(X_train_vectorized)\n",
    "    val_pred = ovo.predict(X_val_vectorized)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    train_score = accuracy_score(train_pred, y_train)\n",
    "    val_score = accuracy_score(val_pred, y_val)\n",
    "    f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "    print(m)\n",
    "    print(f\"train error: {1-train_score}\")\n",
    "    print(f\"test error: {1-val_score}\")\n",
    "    print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59465456",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f1ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of min_df values from 0-10\n",
    "min_df = range(11)\n",
    "\n",
    "# Iterate through min_df values\n",
    "for m in min_df:\n",
    "    #create the vocabulary based on the training data\n",
    "    vect = TfidfVectorizer(min_df=m, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "    #encode the words in X_train and X_val based on the vocabulary\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "    # Define a KNeighborsClassifier model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "    knn.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = knn.predict(X_train_vectorized)\n",
    "    val_pred = knn.predict(X_val_vectorized)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    train_score = accuracy_score(train_pred, y_train)\n",
    "    val_score = accuracy_score(val_pred, y_val)\n",
    "    f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "    print(m)\n",
    "    print(f\"train error: {1-train_score}\")\n",
    "    print(f\"test error: {1-val_score}\")\n",
    "    print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ff305",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590cba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without pruning conditions\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_val based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "# Define DecisionTreeClassifier model\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "dtc.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict\n",
    "train_pred = dtc.predict(X_train_vectorized)\n",
    "val_pred = dtc.predict(X_val_vectorized)\n",
    "  \n",
    "# Accuracy scores\n",
    "train_score = accuracy_score(train_pred, y_train)\n",
    "val_score = accuracy_score(val_pred, y_val)\n",
    "f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "print(f\"train error: {1-train_score}\")\n",
    "print(f\"test error: {1-val_score}\")\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d97399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_depth = [x for x in range(1, 11)]\n",
    "min_samples_split = [x for x in range(2, 11)]\n",
    "min_samples_leaf = [x for x in range(1, 11)]\n",
    "param_grid = {'max_depth': max_depth, 'min_samples_split': min_samples_split, \n",
    "              'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_val based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "# Define a DecisionTreeRegressor model\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Perform random search cv on the decision tree model to determine the best\n",
    "# max_depth, min_samples_split , and min_samples leaf values\n",
    "dtc_cv = RandomizedSearchCV(dtc, param_grid, cv=10, n_jobs=-1, random_state=123)\n",
    "dtc_cv.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# The best parameters\n",
    "best_md = dtc_cv.best_params_['max_depth']\n",
    "best_mss = dtc_cv.best_params_['min_samples_split']\n",
    "best_msl = dtc_cv.best_params_['min_samples_leaf']\n",
    "print('Best max depth: ' + str(best_md))\n",
    "print('Best min samples split: ' + str(best_mss))\n",
    "print('Best min samples leaf: ' + str(best_msl))\n",
    "\n",
    "# Predict\n",
    "train_pred = dtc_cv.predict(X_train_vectorized)\n",
    "val_pred = dtc_cv.predict(X_val_vectorized)\n",
    "    \n",
    "# Accuracy scores\n",
    "train_score = accuracy_score(train_pred, y_train)\n",
    "val_score = accuracy_score(val_pred, y_val)\n",
    "f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "print(f\"train error: {1-train_score}\")\n",
    "print(f\"test error: {1-val_score}\")\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d91b5d9",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde29ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without pruning conditions\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_val based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "# Define a RandomForestClassifier model\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfc.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict\n",
    "train_pred = rfc.predict(X_train_vectorized)\n",
    "val_pred = rfc.predict(X_val_vectorized)\n",
    "    \n",
    "# Accuracy scores\n",
    "train_score = accuracy_score(train_pred, y_train)\n",
    "val_score = accuracy_score(val_pred, y_val)\n",
    "f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "print(f\"train error: {1-train_score}\")\n",
    "print(f\"test error: {1-val_score}\")\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ee8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_depth = [x for x in range(1, 11)]\n",
    "num_trees = [1, 50, 100, 150, 200, 300, 400]\n",
    "param_grid = {'max_depth': max_depth, 'n_estimators': num_trees}\n",
    "\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_val based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "# Define a RandomForestClassifier model\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Perform random search cv on the random forest model to determine the best\n",
    "# n_estimators and max_depth values\n",
    "rfc_cv = RandomizedSearchCV(rfc, param_grid, cv=10, n_jobs=-1, random_state=123)\n",
    "rfc_cv.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# The best parameters\n",
    "best_md = rfc_cv.best_params_['max_depth']\n",
    "best_ne = rfc_cv.best_params_['n_estimators']\n",
    "print('Best max depth: ' + str(best_md))\n",
    "print('Best number of trees: ' + str(best_ne))\n",
    "\n",
    "# Predict\n",
    "train_pred = rfc_cv.predict(X_train_vectorized)\n",
    "val_pred = rfc_cv.predict(X_val_vectorized)\n",
    "    \n",
    "# Accuracy scores\n",
    "train_score = accuracy_score(train_pred, y_train)\n",
    "val_score = accuracy_score(val_pred, y_val)\n",
    "f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "print(f\"train error: {1-train_score}\")\n",
    "print(f\"test error: {1-val_score}\")\n",
    "print(f'F1 score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
