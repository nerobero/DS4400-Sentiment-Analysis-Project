{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd660bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries here: \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, auc, roc_auc_score\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94d7237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37244</th>\n",
       "      <td>jesus</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37245</th>\n",
       "      <td>kya bhai pure saal chutiya banaya modi aur jab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37246</th>\n",
       "      <td>downvote karna tha par upvote hogaya</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37247</th>\n",
       "      <td>haha nice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37248</th>\n",
       "      <td>facebook itself now working bjp’ cell</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37249 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           clean_comment  category\n",
       "0       family mormon have never tried explain them t...         1\n",
       "1      buddhism has very much lot compatible with chr...         1\n",
       "2      seriously don say thing first all they won get...        -1\n",
       "3      what you have learned yours and only yours wha...         0\n",
       "4      for your own benefit you may want read living ...         1\n",
       "...                                                  ...       ...\n",
       "37244                                              jesus         0\n",
       "37245  kya bhai pure saal chutiya banaya modi aur jab...         1\n",
       "37246              downvote karna tha par upvote hogaya          0\n",
       "37247                                         haha nice          1\n",
       "37248             facebook itself now working bjp’ cell          0\n",
       "\n",
       "[37249 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "reddit = pd.read_csv(\"Data/Reddit_Data.csv\")\n",
    "reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd10fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reddit['clean_comment'].fillna(\" \")\n",
    "y = reddit['category']\n",
    "\n",
    "# Split train_data into testing and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, random_state=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af94db1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAASvElEQVR4nO3dbYxc53ne8f9VsnTiuDYpa8soJFUSFe2AChpEmVIsjBaJVZCUG4T6YBg0iop12RBo5DRpgtpS+kGAbRR2E1SNUFsFYymmAkM0oToRkdhmCVmtPzR6GUqObEpWtJUqcwnJXIuU3NaAFMp3P8zDZLxeirszy5l9+f+AwZ5zP8+Z8wzOktectz2pKiRJK9vfGPcAJEnjZxhIkgwDSZJhIEnCMJAkAavHPYBBXXnllbV58+ZxD0OSlpQTJ058t6omZtaXbBhs3ryZbrc77mFI0pKS5IXZ6h4mkiQZBpIkw0CSxBzCIMk9Sc4k+eaM+q8l+VaSk0n+Q1/9tiSTSZ5JsquvvrvVJpPc2lffkuSRVv9CkjUL9eEkSXMzlz2DzwG7+wtJfhHYA/xsVV0L/G6rbwP2Ate2ZT6TZFWSVcCngRuBbcAHW1+ATwF3VNU1wDlg/7AfSpI0P5cMg6r6GnB2RvlfAZ+sqtdanzOtvgc4XFWvVdXzwCSwvb0mq+q5qnodOAzsSRLgvcD9bflDwE3DfSRJ0nwNes7gXcA/bId3/keSv9/qG4BTff2mWu1i9XcCr1TV+Rn1WSU5kKSbpDs9PT3g0CVJMw0aBquBK4AdwL8FjrRv+ZdVVR2sqk5VdSYmfuSeCUnSgAa96WwK+GL1HobwaJIfAFcCp4FNff02thoXqb8MrE2yuu0d9PeXJI3IoHsGfwz8IkCSdwFrgO8CR4G9Sd6SZAuwFXgUeAzY2q4cWkPvJPPRFiYPAe9v77sPeGDAMUlahpLl/VosLrlnkOQ+4BeAK5NMAbcD9wD3tMtNXwf2tf/YTyY5AjwFnAduqao32vt8GDgGrALuqaqTbRUfBQ4n+QTwBHD3An4+SdIcZKk+9rLT6ZR/m0ha/hbTt+fLYdT/BSc5UVWdmXXvQJYkGQaSJMNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRJzCIMk9yQ50x5xObPtt5JUkivbfJLcmWQyyZNJruvruy/Js+21r6/+80m+0Za5M1nuzzWSpMVnLnsGnwN2zywm2QTsBL7dV74R2NpeB4C7Wt8r6D07+XpgO3B7knVtmbuAX+lb7kfWJUm6vC4ZBlX1NeDsLE13AB8B+p/guQe4t3oeBtYmuQrYBRyvqrNVdQ44DuxubW+vqoer9zDme4GbhvpEkqR5G+icQZI9wOmq+vMZTRuAU33zU632ZvWpWeqSpBFaPd8FkrwV+G16h4hGKskBeoefuPrqq0e9eklatgbZM/i7wBbgz5P8b2Aj8HiSnwROA5v6+m5stTerb5ylPquqOlhVnarqTExMDDB0SdJs5h0GVfWNqvrbVbW5qjbTO7RzXVW9BBwFbm5XFe0AXq2qF4FjwM4k69qJ453Asdb2vSQ72lVENwMPLNBnkyTN0VwuLb0P+DPg3Ummkux/k+5fAp4DJoHfB34VoKrOAh8HHmuvj7Uarc9n2zL/C/jyYB9FkjSo9C7iWXo6nU51u91xD0PSZbbc7zwa9X/BSU5UVWdm3TuQJUmGgSTJMJAkYRhIkjAMJEkYBpIkBvhzFNJS46WJ0qW5ZyBJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJuT0D+Z4kZ5J8s6/2O0m+leTJJH+UZG1f221JJpM8k2RXX313q00mubWvviXJI63+hSRrFvDzSZLmYC57Bp8Dds+oHQd+pqr+HvAXwG0ASbYBe4Fr2zKfSbIqySrg08CNwDbgg60vwKeAO6rqGuAcsH+oTyRJmrdLhkFVfQ04O6P236rqfJt9GNjYpvcAh6vqtap6HpgEtrfXZFU9V1WvA4eBPUkCvBe4vy1/CLhpuI8kSZqvhThn8C+AL7fpDcCpvrapVrtY/Z3AK33BcqE+qyQHknSTdKenpxdg6JIkGDIMkvw74Dzw+YUZzpurqoNV1amqzsTExChWKUkrwsAPt0nyz4FfAm6o+qvHa5wGNvV129hqXKT+MrA2yeq2d9DfX5I0IgPtGSTZDXwE+OWq+n5f01Fgb5K3JNkCbAUeBR4DtrYrh9bQO8l8tIXIQ8D72/L7gAcG+yiSpEHN5dLS+4A/A96dZCrJfuA/A38LOJ7k60n+C0BVnQSOAE8BXwFuqao32rf+DwPHgKeBI60vwEeB30wySe8cwt0L+gklSZeUWqIPUO10OtXtdsc9DC0BPgN5aXP7LawkJ6qqM7PuHciSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIElibo+9vCfJmSTf7KtdkeR4kmfbz3WtniR3JplM8mSS6/qW2df6P5tkX1/955N8oy1zZ7Lcn2skSYvPXPYMPgfsnlG7FXiwqrYCD7Z5gBuBre11ALgLeuEB3A5cD2wHbr8QIK3Pr/QtN3NdkqTL7JJhUFVfA87OKO8BDrXpQ8BNffV7q+dhYG2Sq4BdwPGqOltV54DjwO7W9vaqerh6D2O+t++9JEkjMug5g/VV9WKbfglY36Y3AKf6+k212pvVp2apzyrJgSTdJN3p6ekBhy5JmmnoE8jtG30twFjmsq6DVdWpqs7ExMQoVilJK8KgYfCddoiH9vNMq58GNvX129hqb1bfOEtdkjRCg4bBUeDCFUH7gAf66je3q4p2AK+2w0nHgJ1J1rUTxzuBY63te0l2tKuIbu57L0nSiKy+VIck9wG/AFyZZIreVUGfBI4k2Q+8AHygdf8S8D5gEvg+8CGAqjqb5OPAY63fx6rqwknpX6V3xdKPA19uL0nSCKV3yH/p6XQ61e12xz0MLQHL/c6VJfpPeM7cfgsryYmq6syseweyJMkwkCQZBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSGDIMkvybJCeTfDPJfUl+LMmWJI8kmUzyhSRrWt+3tPnJ1r65731ua/Vnkuwa8jNdFsnyfUnSwGGQZAPwr4FOVf0MsArYC3wKuKOqrgHOAfvbIvuBc61+R+tHkm1tuWuB3cBnkqwadFySpPkb9jDRauDHk6wG3gq8CLwXuL+1HwJuatN72jyt/YYkafXDVfVaVT0PTALbhxyXJGkeBg6DqjoN/C7wbXoh8CpwAnilqs63blPAhja9ATjVlj3f+r+zvz7LMj8kyYEk3STd6enpQYcuSZphmMNE6+h9q98C/BTwE/QO81w2VXWwqjpV1ZmYmLicq5KkFWWYw0T/GHi+qqar6i+BLwLvAda2w0YAG4HTbfo0sAmgtb8DeLm/PssykqQRGCYMvg3sSPLWduz/BuAp4CHg/a3PPuCBNn20zdPav1pV1ep729VGW4CtwKNDjEuSNE+rL91ldlX1SJL7gceB88ATwEHgT4HDST7Rane3Re4G/jDJJHCW3hVEVNXJJEfoBcl54JaqemPQcUmS5i+9L+dLT6fTqW63O7L1Lefr8Zfor8CcLedtB26/pW7U2y/JiarqzKx7B7IkyTCQJBkGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIYMgySrE1yf5JvJXk6yT9IckWS40mebT/Xtb5JcmeSySRPJrmu7332tf7PJtl38TVKki6HYfcMfg/4SlX9NPCzwNPArcCDVbUVeLDNA9xI72H3W4EDwF0ASa4AbgeuB7YDt18IEEnSaAwcBkneAfwj2gPvq+r1qnoF2AMcat0OATe16T3AvdXzMLA2yVXALuB4VZ2tqnPAcWD3oOOSJM3fMHsGW4Bp4A+SPJHks0l+AlhfVS+2Pi8B69v0BuBU3/JTrXax+o9IciBJN0l3enp6iKFLkvoNEwargeuAu6rq54D/x18fEgKgqgqoIdbxQ6rqYFV1qqozMTGxUG8rSSveMGEwBUxV1SNt/n564fCddviH9vNMaz8NbOpbfmOrXawuSRqRgcOgql4CTiV5dyvdADwFHAUuXBG0D3igTR8Fbm5XFe0AXm2Hk44BO5OsayeOd7aaJGlEVg+5/K8Bn0+yBngO+BC9gDmSZD/wAvCB1vdLwPuASeD7rS9VdTbJx4HHWr+PVdXZIcclSZqH9A7rLz2dTqe63e7I1peMbFUjt0R/BeZsOW87cPstdaPefklOVFVnZt07kCVJhoEkyTCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAksQChEGSVUmeSPInbX5LkkeSTCb5QnskJkne0uYnW/vmvve4rdWfSbJr2DFJkuZnIfYMfh14um/+U8AdVXUNcA7Y3+r7gXOtfkfrR5JtwF7gWmA38JkkqxZgXJKkORoqDJJsBP4J8Nk2H+C9wP2tyyHgpja9p83T2m9o/fcAh6vqtap6HpgEtg8zLknS/Ay7Z/CfgI8AP2jz7wReqarzbX4K2NCmNwCnAFr7q63/X9VnWeaHJDmQpJukOz09PeTQJUkXDBwGSX4JOFNVJxZwPG+qqg5WVaeqOhMTE6NarSQte6uHWPY9wC8neR/wY8Dbgd8D1iZZ3b79bwROt/6ngU3AVJLVwDuAl/vqF/QvI0kagYH3DKrqtqraWFWb6Z0A/mpV/VPgIeD9rds+4IE2fbTN09q/WlXV6nvb1UZbgK3Ao4OOS5I0f8PsGVzMR4HDST4BPAHc3ep3A3+YZBI4Sy9AqKqTSY4ATwHngVuq6o3LMC5J0kWk9+V86el0OtXtdke2vmRkqxq5JforMGfLeduB22+pG/X2S3Kiqjoz696BLEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJIYIgySbkjyU5KkkJ5P8eqtfkeR4kmfbz3WtniR3JplM8mSS6/rea1/r/2ySfRdbpyTp8hhmz+A88FtVtQ3YAdySZBtwK/BgVW0FHmzzADfSe9j9VuAAcBf0wgO4Hbge2A7cfiFAJEmjMXAYVNWLVfV4m/4/wNPABmAPcKh1OwTc1Kb3APdWz8PA2iRXAbuA41V1tqrOAceB3YOOS5I0fwtyziDJZuDngEeA9VX1Ymt6CVjfpjcAp/oWm2q1i9UlSSMydBgkeRvwX4HfqKrv9bdVVQE17Dr61nUgSTdJd3p6eqHeVpJWvKHCIMnfpBcEn6+qL7byd9rhH9rPM61+GtjUt/jGVrtY/UdU1cGq6lRVZ2JiYpihS5L6DHM1UYC7gaer6j/2NR0FLlwRtA94oK9+c7uqaAfwajucdAzYmWRdO3G8s9UkSSOyeohl3wP8M+AbSb7ear8NfBI4kmQ/8ALwgdb2JeB9wCTwfeBDAFV1NsnHgcdav49V1dkhxiVJmqf0DusvPZ1Op7rd7sjWl4xsVSO3RH8F5mw5bztw+y11o95+SU5UVWdm3TuQJUmGgSTJMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgSWIRhUGS3UmeSTKZ5NZxj0eSVpJFEQZJVgGfBm4EtgEfTLJtvKOSpJVjUYQBsB2YrKrnqup14DCwZ8xjkqQVY/W4B9BsAE71zU8B18/slOQAcKDN/t8kz4xgbONyJfDdUawoGcVaVpSRbTtw+10Gy337/Z3ZioslDOakqg4CB8c9jlFI0q2qzrjHoflz2y1tK3X7LZbDRKeBTX3zG1tNkjQCiyUMHgO2JtmSZA2wFzg65jFJ0oqxKA4TVdX5JB8GjgGrgHuq6uSYhzVuK+Jw2DLltlvaVuT2S1WNewySpDFbLIeJJEljZBhIkgwDSdIiOYEsLWVJfpreHfMbWuk0cLSqnh7fqKT5cc9gEUvytnGPQW8uyUfp/fmUAI+2V4D7/IOLS1uSD417DKPk1USLWJJvV9XV4x6HLi7JXwDXVtVfzqivAU5W1dbxjEzDWmn//jxMNGZJfvNiTYB7BovfD4CfAl6YUb+qtWkRS/LkxZqA9aMcy7gZBuP374HfAc7P0uZhvMXvN4AHkzzLX/+xxauBa4APj2tQmrP1wC7g3Ix6gP85+uGMj2Ewfo8Df1xVJ2Y2JPmXYxiP5qGqvpLkXfT+DHv/CeTHquqN8Y1Mc/QnwNuq6uszG5L895GPZow8ZzBmSd4NvFxV3+2r/WRVvZRkfVV9Z4zDk7RCGAaLUJLHq+q6cY9D0srhMenFyceVSBopw2Bx+v1xD0DSyuJhIkmSewaSJMNAkoRhIEnCMJAkAf8f0LdvmVrpK+kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Distribution\n",
    "reddit.category.value_counts().sort_index().plot(kind='bar',color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426728ce",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e4b81cb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasmineliu0114/opt/anaconda3/lib/python3.8/site-packages/sklearn/naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn('alpha too small will result in numeric errors, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha: 0.0\n",
      "Training score: 0.9907379442263163\n",
      "Val score: 0.568993288590604\n",
      "F1 score: 0.5111081265635883\n",
      "alpha: 0.1\n",
      "Training score: 0.9810396322024229\n",
      "Val score: 0.5828187919463087\n",
      "F1 score: 0.5009955478662155\n",
      "alpha: 0.2\n",
      "Training score: 0.9596966341152388\n",
      "Val score: 0.5481879194630872\n",
      "F1 score: 0.4368847167738066\n",
      "alpha: 0.30000000000000004\n",
      "Training score: 0.907849256686466\n",
      "Val score: 0.5315436241610738\n",
      "F1 score: 0.40584530087769727\n",
      "alpha: 0.4\n",
      "Training score: 0.8442565186751233\n",
      "Val score: 0.5220134228187919\n",
      "F1 score: 0.38807475995179547\n",
      "alpha: 0.5\n",
      "Training score: 0.7903285345145811\n",
      "Val score: 0.5159731543624161\n",
      "F1 score: 0.37694960201116195\n",
      "alpha: 0.6000000000000001\n",
      "Training score: 0.7468371421859794\n",
      "Val score: 0.5096644295302013\n",
      "F1 score: 0.36492880050952947\n",
      "alpha: 0.7000000000000001\n",
      "Training score: 0.710527198899292\n",
      "Val score: 0.505503355704698\n",
      "F1 score: 0.3574286850799983\n",
      "alpha: 0.8\n",
      "Training score: 0.682774589751334\n",
      "Val score: 0.5009395973154362\n",
      "F1 score: 0.34970585216598016\n",
      "alpha: 0.9\n",
      "Training score: 0.6623376623376623\n",
      "Val score: 0.4993288590604027\n",
      "F1 score: 0.3460819320493454\n",
      "alpha: 1.0\n",
      "Training score: 0.645659250310413\n",
      "Val score: 0.4975838926174497\n",
      "F1 score: 0.3425969307301641\n"
     ]
    }
   ],
   "source": [
    "# Range of alpha values from 0-1\n",
    "alpha = np.arange(0,1.1,0.1)\n",
    "\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_val based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "#Iterate through each alpha value\n",
    "for a in alpha:\n",
    "    # Define a MultinomialNB model with current alpha value\n",
    "    mnb = MultinomialNB(alpha = a).fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = mnb.predict(X_train_vectorized)\n",
    "    val_pred = mnb.predict(X_val_vectorized)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    train_error = accuracy_score(y_train, train_pred)\n",
    "    val_error = accuracy_score(y_val, val_pred)\n",
    "    f1 = f1_score(y_val, val_pred, average='macro')\n",
    "    \n",
    "    print(f'alpha: {a}')\n",
    "    print(f'Training score: {train_error}')\n",
    "    print(f'Val score: {val_error}')\n",
    "    print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8411b49",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d5e90",
   "metadata": {},
   "source": [
    "### One-vs-Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d61750c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train error: 0.09272123225611595\n",
      "test error: 0.207248322147651\n",
      "F1 score: 0.7604993785340816\n",
      "1\n",
      "train error: 0.09272123225611595\n",
      "test error: 0.207248322147651\n",
      "F1 score: 0.7604993785340816\n",
      "2\n",
      "train error: 0.09007013658176444\n",
      "test error: 0.17919463087248322\n",
      "F1 score: 0.7907751900651016\n",
      "3\n",
      "train error: 0.09124467264002145\n",
      "test error: 0.1719463087248322\n",
      "F1 score: 0.798881716308432\n",
      "4\n",
      "train error: 0.09201651062116178\n",
      "test error: 0.16697986577181212\n",
      "F1 score: 0.8045748098109895\n",
      "5\n",
      "train error: 0.09312393033323263\n",
      "test error: 0.16389261744966444\n",
      "F1 score: 0.8089843591249545\n",
      "6\n",
      "train error: 0.0938286519681869\n",
      "test error: 0.1605369127516778\n",
      "F1 score: 0.8124134451547979\n",
      "7\n",
      "train error: 0.09473472264169935\n",
      "test error: 0.15892617449664426\n",
      "F1 score: 0.8155688381392278\n",
      "8\n",
      "train error: 0.09621128225779385\n",
      "test error: 0.1573154362416107\n",
      "F1 score: 0.8174608004045939\n",
      "9\n",
      "train error: 0.09664753850800367\n",
      "test error: 0.15382550335570466\n",
      "F1 score: 0.8210873192590604\n",
      "10\n",
      "train error: 0.09651330581563144\n",
      "test error: 0.15288590604026842\n",
      "F1 score: 0.822511229662286\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression, One-vs-Rest method \n",
    "# Range of min_df values from 0-10\n",
    "min_df = range(11)\n",
    "\n",
    "# Iterate through min_df values\n",
    "for m in min_df:\n",
    "    #create the vocabulary based on the training data\n",
    "    vect = TfidfVectorizer(min_df=m, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "    #encode the words in X_train and X_val based on the vocabulary\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "    # Pass a LogisticRegression model through a OneVsRestClassifier object\n",
    "    ovr = OneVsRestClassifier(LogisticRegression(n_jobs = -1))\n",
    "    ovr.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = ovr.predict(X_train_vectorized)\n",
    "    val_pred = ovr.predict(X_val_vectorized)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    train_score = accuracy_score(train_pred, y_train)\n",
    "    val_score = accuracy_score(val_pred, y_val)\n",
    "    f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "    print(m)\n",
    "    print(f\"train error: {1-train_score}\")\n",
    "    print(f\"test error: {1-val_score}\")\n",
    "    print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2cf992",
   "metadata": {},
   "source": [
    "### One-vs-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "818eeede",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train error: 0.11611127890197659\n",
      "test error: 0.2303355704697987\n",
      "F1 score: 0.736031470758998\n",
      "1\n",
      "train error: 0.11611127890197659\n",
      "test error: 0.2303355704697987\n",
      "F1 score: 0.736031470758998\n",
      "2\n",
      "train error: 0.10537266351219843\n",
      "test error: 0.19597315436241614\n",
      "F1 score: 0.7723623098098177\n",
      "3\n",
      "train error: 0.10413101110775524\n",
      "test error: 0.18644295302013425\n",
      "F1 score: 0.7836559905854571\n",
      "4\n",
      "train error: 0.10392966206919696\n",
      "test error: 0.18201342281879196\n",
      "F1 score: 0.7893975932432641\n",
      "5\n",
      "train error: 0.10490284908889558\n",
      "test error: 0.17946308724832216\n",
      "F1 score: 0.7923192452356268\n",
      "6\n",
      "train error: 0.10574180341622197\n",
      "test error: 0.17624161073825506\n",
      "F1 score: 0.7959511071068656\n",
      "7\n",
      "train error: 0.10530554716601226\n",
      "test error: 0.17355704697986574\n",
      "F1 score: 0.7990915910585322\n",
      "8\n",
      "train error: 0.10574180341622197\n",
      "test error: 0.170738255033557\n",
      "F1 score: 0.8025340856163403\n",
      "9\n",
      "train error: 0.10631229235880402\n",
      "test error: 0.16832214765100673\n",
      "F1 score: 0.8057203007064307\n",
      "10\n",
      "train error: 0.10527198899291923\n",
      "test error: 0.16577181208053693\n",
      "F1 score: 0.8084570996053896\n"
     ]
    }
   ],
   "source": [
    "# Range of min_df values from 0-10\n",
    "min_df = range(11)\n",
    "\n",
    "# Iterate through min_df values\n",
    "for m in min_df:\n",
    "    #create the vocabulary based on the training data\n",
    "    vect = TfidfVectorizer(min_df=m, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "    #encode the words in X_train and X_val based on the vocabulary\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "    # Pass a LogisticRegression model through a OneVsOneClassifier object\n",
    "    ovo = OneVsOneClassifier(LogisticRegression(n_jobs = -1))\n",
    "    ovo.fit(X_train_vectorized, y_train)\n",
    "    ovo.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = ovo.predict(X_train_vectorized)\n",
    "    val_pred = ovo.predict(X_val_vectorized)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    train_score = accuracy_score(train_pred, y_train)\n",
    "    val_score = accuracy_score(val_pred, y_val)\n",
    "    f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "    print(m)\n",
    "    print(f\"train error: {1-train_score}\")\n",
    "    print(f\"test error: {1-val_score}\")\n",
    "    print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a53f27",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83faac1b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train error: 0.6351891003053793\n",
      "test error: 0.6316778523489932\n",
      "F1 score: 0.19826696745751912\n",
      "1\n",
      "train error: 0.6351891003053793\n",
      "test error: 0.6316778523489932\n",
      "F1 score: 0.19826696745751912\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2c0306dad5c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mval_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_vectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             chunked_results = list(pairwise_distances_chunked(\n\u001b[0m\u001b[1;32m    706\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meffective_metric_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   1621\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         D_chunk = pairwise_distances(X_chunk, Y, metric=metric,\n\u001b[0m\u001b[1;32m   1624\u001b[0m                                      n_jobs=n_jobs, **kwds)\n\u001b[1;32m   1625\u001b[0m         if ((X is Y or Y is None)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    621\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    622\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/scipy/sparse/_compressed.py\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matmat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         fn(M, N, np.asarray(self.indptr, dtype=idx_dtype),\n\u001b[0m\u001b[1;32m    534\u001b[0m            \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Range of min_df values from 0-10\n",
    "min_df = range(11)\n",
    "\n",
    "# Iterate through min_df values\n",
    "for m in min_df:\n",
    "    #create the vocabulary based on the training data\n",
    "    vect = TfidfVectorizer(min_df=m, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "    #encode the words in X_train and X_val based on the vocabulary\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "    # Define a KNeighborsClassifier model\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "    knn.fit(X_train_vectorized, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    train_pred = knn.predict(X_train_vectorized)\n",
    "    val_pred = knn.predict(X_val_vectorized)\n",
    "    \n",
    "    # Accuracy scores\n",
    "    train_score = accuracy_score(train_pred, y_train)\n",
    "    val_score = accuracy_score(val_pred, y_val)\n",
    "    f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "    print(m)\n",
    "    print(f\"train error: {1-train_score}\")\n",
    "    print(f\"test error: {1-val_score}\")\n",
    "    print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b1d5dc",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c86eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without pruning conditions\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_val based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "# Define DecisionTreeClassifier model\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "dtc.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict\n",
    "train_pred = dtc.predict(X_train_vectorized)\n",
    "val_pred = dtc.predict(X_val_vectorized)\n",
    "    \n",
    "# Accuracy scores\n",
    "train_score = accuracy_score(train_pred, y_train)\n",
    "val_score = accuracy_score(val_pred, y_val)\n",
    "f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "print(f\"train error: {1-train_score}\")\n",
    "print(f\"test error: {1-val_score}\")\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8163c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_depth = [x for x in range(1, 11)]\n",
    "min_samples_split = [x for x in range(2, 11)]\n",
    "min_samples_leaf = [x for x in range(1, 11)]\n",
    "param_grid = {'max_depth': max_depth, 'min_samples_split': min_samples_split, \n",
    "              'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_val based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "# Define a DecisionTreeRegressor model\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Perform random search cv on the decision tree model to determine the best\n",
    "# max_depth, min_samples_split , and min_samples leaf values\n",
    "dtc_cv = RandomizedSearchCV(dtc, param_grid, cv=10, n_jobs=-1, random_state=123)\n",
    "dtc_cv.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# The best parameters\n",
    "best_md = dtc_cv.best_params_['max_depth']\n",
    "best_mss = dtc_cv.best_params_['min_samples_split']\n",
    "best_msl = dtc_cv.best_params_['min_samples_leaf']\n",
    "print('Best max depth: ' + str(best_md))\n",
    "print('Best min samples split: ' + str(best_mss))\n",
    "print('Best min samples leaf: ' + str(best_msl))\n",
    "\n",
    "# Predict\n",
    "train_pred = dtc_cv.predict(X_train_vectorized)\n",
    "val_pred = dtc_cv.predict(X_val_vectorized)\n",
    "   \n",
    "# Accuracy scores\n",
    "train_score = accuracy_score(train_pred, y_train)\n",
    "val_score = accuracy_score(val_pred, y_val)\n",
    "f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "print(f\"train error: {1-train_score}\")\n",
    "print(f\"test error: {1-val_score}\")\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7a4a4",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without pruning conditions\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_val based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "# Define a RandomForestClassifier model\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "rfc.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict\n",
    "train_pred = rfc.predict(X_train_vectorized)\n",
    "val_pred = rfc.predict(X_val_vectorized)\n",
    " \n",
    "# Accuracy scores\n",
    "train_score = accuracy_score(train_pred, y_train)\n",
    "val_score = accuracy_score(val_pred, y_val)\n",
    "f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "print(f\"train error: {1-train_score}\")\n",
    "print(f\"test error: {1-val_score}\")\n",
    "print(f'F1 score: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_depth = [x for x in range(1, 11)]\n",
    "num_trees = [1, 50, 100, 150, 200, 300, 400]\n",
    "param_grid = {'max_depth': max_depth, 'n_estimators': num_trees}\n",
    "\n",
    "#create the vocabulary based on the training data\n",
    "vect = TfidfVectorizer(ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "#encode the words in X_train and X_val based on the vocabulary\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "# Define a RandomForestClassifier model\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Perform random search cv on the random forest model to determine the best\n",
    "# n_estimators and max_depth values\n",
    "rfc_cv = RandomizedSearchCV(rfc, param_grid, cv=10, n_jobs=-1, random_state=123)\n",
    "rfc_cv.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# The best parameters\n",
    "best_md = rfc_cv.best_params_['max_depth']\n",
    "best_ne = rfc_cv.best_params_['n_estimators']\n",
    "print('Best max depth: ' + str(best_md))\n",
    "print('Best number of trees: ' + str(best_ne))\n",
    "\n",
    "# Predict\n",
    "train_pred = rfc_cv.predict(X_train_vectorized)\n",
    "val_pred = rfc_cv.predict(X_val_vectorized)\n",
    "\n",
    "# Accuracy scores\n",
    "train_score = accuracy_score(train_pred, y_train)\n",
    "val_score = accuracy_score(val_pred, y_val)\n",
    "f1 = f1_score(y_val, val_pred, average='macro')\n",
    "\n",
    "print(f\"train error: {1-train_score}\")\n",
    "print(f\"test error: {1-val_score}\")\n",
    "print(f'F1 score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
