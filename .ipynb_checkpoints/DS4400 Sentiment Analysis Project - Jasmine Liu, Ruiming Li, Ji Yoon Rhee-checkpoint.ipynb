{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1a32306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries here: \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5831f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training and testing data from tsv file\n",
    "train_data = pd.read_csv(\"Data/kaggle_train.tsv\", sep='\\t')\n",
    "test_data = pd.read_csv(\"Data/kaggle_test.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac4e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data['Phrase']\n",
    "y = train_data['Sentiment']\n",
    "\n",
    "X_test = test_data['Phrase']\n",
    "\n",
    "# Split train_data into testing and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, random_state=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0d3bf2",
   "metadata": {},
   "source": [
    "### MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d61a73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best min_df: 0\n",
      "Best alpha: 0.4\n",
      "Best train score: 0.7463956170703575\n",
      "Best validation score: 0.6323529411764706\n"
     ]
    }
   ],
   "source": [
    "# Range of min_df values from 0-10\n",
    "min_df = range(11)\n",
    "# Range of alpha values from 0-1 by increments of 0.1\n",
    "alphas = np.arange(0, 1, 0.1)\n",
    "alphas = np.delete(alphas, 0)\n",
    "\n",
    "# Keep track of the best min_df, alpha, train score, and validation score\n",
    "best_md = None\n",
    "best_alpha = None\n",
    "best_train_score = 0\n",
    "best_val_score = 0\n",
    "\n",
    "# Iterate through min_df and alpha values to find the best combination that produces the highest validation score\n",
    "for m in min_df:\n",
    "    #create the vocabulary based on the training data\n",
    "    vect = TfidfVectorizer(min_df=m, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "    #encode the words in X_train and X_val based on the vocabulary\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_val_vectorized = vect.transform(X_val)\n",
    "    \n",
    "    for a in alphas:\n",
    "        # Train a MultinomialNB model and get the validation score\n",
    "        mnb = MultinomialNB(alpha = a).fit(X_train_vectorized, y_train)\n",
    "        score = mnb.score(X_val_vectorized, y_val)\n",
    "        \n",
    "        # If this model has a higher val score, set the best parameters/scores to the current parameters/scores\n",
    "        if score > best_val_score:\n",
    "            best_md = m\n",
    "            best_alpha = a\n",
    "            best_val_score = score\n",
    "            best_train_score = mnb.score(X_train_vectorized, y_train)\n",
    "            \n",
    "print(f\"Best min_df: {best_md}\")\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best train score: {best_train_score}\")\n",
    "print(f\"Best validation score: {best_val_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f1ceae",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3caa3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "train score: 0.7260508778674869\n",
      "validation score: 0.6438228886325772\n",
      "1\n",
      "train score: 0.7260508778674869\n",
      "validation score: 0.6438228886325772\n",
      "2\n",
      "train score: 0.7343649878251954\n",
      "validation score: 0.6501025246699987\n",
      "3\n",
      "train score: 0.7294710367807253\n",
      "validation score: 0.6456491093169294\n",
      "4\n",
      "train score: 0.7319220171728822\n",
      "validation score: 0.6486928104575164\n",
      "5\n",
      "train score: 0.7240164039471998\n",
      "validation score: 0.6461937716262975\n",
      "6\n",
      "train score: 0.7167916186082276\n",
      "validation score: 0.644239395104447\n",
      "7\n",
      "train score: 0.7151976803793413\n",
      "validation score: 0.6457772651544278\n",
      "8\n",
      "train score: 0.710239651416122\n",
      "validation score: 0.6461937716262975\n",
      "9\n",
      "train score: 0.7051454568755606\n",
      "validation score: 0.645200563885685\n",
      "10\n",
      "train score: 0.7040881712161989\n",
      "validation score: 0.64439958990132\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "# Range of min_df values from 0-10\n",
    "min_df = range(11)\n",
    "\n",
    "for m in min_df:\n",
    "    #create the vocabulary based on the training data\n",
    "    vect = TfidfVectorizer(min_df=m, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "    #encode the words in X_train and X_val based on the vocabulary\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "    lr = LogisticRegression(n_jobs=-1)\n",
    "    lr.fit(X_train_vectorized, y_train)\n",
    "    train_score = lr.score(X_train_vectorized, y_train)\n",
    "    val_score = lr.score(X_val_vectorized, y_val)\n",
    "\n",
    "    print(m)\n",
    "    print(f\"train score: {train_score}\")\n",
    "    print(f\"validation score: {val_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4e729",
   "metadata": {},
   "source": [
    "### Decision Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5134373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of min_df values from 0-10\n",
    "min_df = range(11)\n",
    "\n",
    "for m in min_df:\n",
    "    #create the vocabulary based on the training data\n",
    "    vect = TfidfVectorizer(min_df=m, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "    #encode the words in X_train and X_val based on the vocabulary\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_val_vectorized = vect.transform(X_val)\n",
    "\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    dtc.fit(X_train_vectorized, y_train)\n",
    "    train_score = dtc.score(X_train_vectorized, y_train)\n",
    "    val_score = dtc.score(X_val_vectorized, y_val)\n",
    "\n",
    "    print(m)\n",
    "    print(f\"train score: {train_score}\")\n",
    "    print(f\"validation score: {val_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
